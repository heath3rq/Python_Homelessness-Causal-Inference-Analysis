{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Processing HIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_coc = {'36005': 'NY-600',\n",
    "            '36047': 'NY-600',\n",
    "            '36061': 'NY-600',\n",
    "            '36081': 'NY-600',\n",
    "            '36085': 'NY-600',\n",
    "            #  LA CoC excludes the cities of Glendale, Pasadena and Long Beach\n",
    "            '06037': 'CA-600',\n",
    "            '26163': 'MI-501',\n",
    "            '48201': 'TX-700',\n",
    "            '48339': 'TX-700',\n",
    "            '48157': 'TX-700',\n",
    "            '08001': 'CO-503',\n",
    "            '08005': 'CO-503',\n",
    "            '08013': 'CO-503',\n",
    "            '08014': 'CO-503',\n",
    "            '08031': 'CO-503',\n",
    "            '08035': 'CO-503',\n",
    "            '08059': 'CO-503',\n",
    "            '04013': 'AZ-502',\n",
    "            '53033': 'WA-500',\n",
    "            '42101': 'PA-500',\n",
    "            '32003': 'NV-500',\n",
    "            '06073': 'CA-601',\n",
    "            '06085': 'CA-500',\n",
    "            '06071': 'CA-609',\n",
    "            '05049': 'GA-500',\n",
    "            '12057': 'FL-501',\n",
    "            '17031': 'IL-510',\n",
    "            '06075': 'CA-501',\n",
    "            '11001': 'DC-500',\n",
    "            '48453': 'TX-503',\n",
    "            '25025': 'MA-500',\n",
    "            }\n",
    "uniquecoc = set(val for val in fips_coc.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "hic_path = 'https://www.huduser.gov/portal/sites/default/files/xls/2007-2022-HIC-Counts-by-CoC.xlsx'\n",
    "hic_2022 = pd.read_excel(hic_path, sheet_name='2022', header=1)\n",
    "hic_2021 = pd.read_excel(hic_path, sheet_name='2021', header=1)\n",
    "hic_2020 = pd.read_excel(hic_path, sheet_name='2020', header=1)\n",
    "hic_2019 = pd.read_excel(hic_path, sheet_name='2019', header=1)\n",
    "hic_2018 = pd.read_excel(hic_path, sheet_name='2018', header=1)\n",
    "hic_2017 = pd.read_excel(hic_path, sheet_name='2017', header=1)\n",
    "hic_2016 = pd.read_excel(hic_path, sheet_name='2016', header=1)\n",
    "hic_2015 = pd.read_excel(hic_path, sheet_name='2015', header=1)\n",
    "hic_2014 = pd.read_excel(hic_path, sheet_name='2014', header=1)\n",
    "hic_2013 = pd.read_excel(hic_path, sheet_name='2013', header=1)\n",
    "hic_2012 = pd.read_excel(hic_path, sheet_name='2012', header=1)\n",
    "hic_2011 = pd.read_excel(hic_path, sheet_name='2011', header=1)\n",
    "hic_2010 = pd.read_excel(hic_path, sheet_name='2010', header=1)\n",
    "hic_2009 = pd.read_excel(hic_path, sheet_name='2009', header=1)\n",
    "hic_2008 = pd.read_excel(hic_path, sheet_name='2008', header=1)\n",
    "hic_2007 = pd.read_excel(hic_path, sheet_name='2007', header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hic_1(df: object, year: int) -> object:\n",
    "    \"\"\"Process Dataframe from 2014-2022.\"\"\"\n",
    "    assert (df['CoC Number'].duplicated() == True).any() == False\n",
    "    df[\"year\"] = year\n",
    "    df[\"permanent_housing\"] = df['Total Year-Round Beds (PSH)'] + \\\n",
    "        df['Total Year-Round Beds (OPH)']\n",
    "    hic_ph = df.loc[df['CoC Number'].isin(\n",
    "        uniquecoc), ['CoC Number', 'year', 'permanent_housing']].reset_index(drop='index')\n",
    "    return hic_ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hic_2(df: object, year: int) -> object:\n",
    "    \"\"\"Process Dataframe for 2013.\"\"\"\n",
    "    assert (df['CoC Number'].duplicated() == True).any() == False\n",
    "    df[\"year\"] = year\n",
    "    df[\"permanent_housing\"] = df['Total PSH Beds']\n",
    "    hic_ph = df.loc[df['CoC Number'].isin(\n",
    "        uniquecoc), ['CoC Number', 'year', 'permanent_housing']].reset_index(drop='index')\n",
    "    return hic_ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hic_3(df: object, year: int) -> object:\n",
    "    \"\"\"Process Dataframe for 2007-2012.\"\"\"\n",
    "    assert (df['CoC'].duplicated() == True).any() == False\n",
    "    df[\"year\"] = year\n",
    "    df[\"permanent_housing\"] = df['Total PSH Beds']\n",
    "    hic_ph = df.loc[df['CoC'].isin(\n",
    "        uniquecoc), ['CoC', 'year', 'permanent_housing']].reset_index(drop='index').rename(columns={'CoC': 'CoC Number'})\n",
    "    return hic_ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hic_2022 = process_hic_1(hic_2022, 2022)\n",
    "hic_2021 = process_hic_1(hic_2021, 2021)\n",
    "hic_2020 = process_hic_1(hic_2020, 2020)\n",
    "hic_2019 = process_hic_1(hic_2019, 2019)\n",
    "hic_2018 = process_hic_1(hic_2018, 2018)\n",
    "hic_2017 = process_hic_1(hic_2017, 2017)\n",
    "hic_2016 = process_hic_1(hic_2016, 2016)\n",
    "hic_2015 = process_hic_1(hic_2015, 2015)\n",
    "hic_2014 = process_hic_1(hic_2014, 2014)\n",
    "hic_2013 = process_hic_2(hic_2013, 2013)\n",
    "hic_2012 = process_hic_3(hic_2012, 2012)\n",
    "hic_2011 = process_hic_3(hic_2011, 2011)\n",
    "hic_2010 = process_hic_3(hic_2010, 2010)\n",
    "hic_2009 = process_hic_3(hic_2009, 2009)\n",
    "hic_2008 = process_hic_3(hic_2008, 2008)\n",
    "hic_2007 = process_hic_3(hic_2007, 2007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "\n",
    "# # define list of DataFrames\n",
    "hic = [\n",
    "    hic_2022,\n",
    "    hic_2021,\n",
    "    hic_2020,\n",
    "    hic_2019,\n",
    "    hic_2018,\n",
    "    hic_2017,\n",
    "    hic_2016,\n",
    "    hic_2015,\n",
    "    hic_2014,\n",
    "    hic_2013,\n",
    "    hic_2012,\n",
    "    hic_2011,\n",
    "    hic_2010,\n",
    "    hic_2009,\n",
    "    hic_2008,\n",
    "    hic_2007,]\n",
    "\n",
    "# Concatenate the dataframes\n",
    "hic_combined = pd.concat(hic, ignore_index=True)\n",
    "\n",
    "# Confirm the resulting dataframe has no duplicates and null values\n",
    "\n",
    "\n",
    "def check_dup_null(df: object) -> None:\n",
    "    assert df.duplicated().sum() == 0, \"Found duplicates in the dataframe.\"\n",
    "    assert df.isnull().sum().sum() == 0, \"Found null values in the dataframe.\"\n",
    "    assert (df.groupby(\"CoC Number\").count() == 16).all().all(\n",
    "    ) == True, \"Certain years are missing. Review yearly data for each CoC.\"  # 16 years\n",
    "\n",
    "\n",
    "check_dup_null(hic_combined)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Processing PIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_path = 'https://www.huduser.gov/portal/sites/default/files/xls/2007-2022-PIT-Counts-by-CoC.xlsx'\n",
    "pit_2022 = pd.read_excel(pit_path, sheet_name='2022', usecols='A,D', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2022': 'homeless'})\n",
    "pit_2021 = pd.read_excel(pit_path, sheet_name='2021', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2021': 'homeless'})\n",
    "pit_2020 = pd.read_excel(pit_path, sheet_name='2020', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2020': 'homeless'})\n",
    "pit_2019 = pd.read_excel(pit_path, sheet_name='2019', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2019': 'homeless'})\n",
    "pit_2018 = pd.read_excel(pit_path, sheet_name='2018', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2018': 'homeless'})\n",
    "pit_2017 = pd.read_excel(pit_path, sheet_name='2017', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2017': 'homeless'})\n",
    "pit_2016 = pd.read_excel(pit_path, sheet_name='2016', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2016': 'homeless'})\n",
    "pit_2015 = pd.read_excel(pit_path, sheet_name='2015', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2015': 'homeless'})\n",
    "pit_2014 = pd.read_excel(pit_path, sheet_name='2014', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2014': 'homeless'})\n",
    "pit_2013 = pd.read_excel(pit_path, sheet_name='2013', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2013': 'homeless'})\n",
    "pit_2012 = pd.read_excel(pit_path, sheet_name='2012', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2012': 'homeless'})\n",
    "pit_2011 = pd.read_excel(pit_path, sheet_name='2011', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2011': 'homeless'})\n",
    "pit_2010 = pd.read_excel(pit_path, sheet_name='2010', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2010': 'homeless'})\n",
    "pit_2009 = pd.read_excel(pit_path, sheet_name='2009', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2009': 'homeless'})\n",
    "pit_2008 = pd.read_excel(pit_path, sheet_name='2008', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2008': 'homeless'})\n",
    "pit_2007 = pd.read_excel(pit_path, sheet_name='2007', usecols='A,C', skipfooter=3).rename(\n",
    "    columns={'Overall Homeless, 2007': 'homeless'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pit(df: object, year: int) -> object:\n",
    "    \"\"\"Process Dataframe for 2007-2012.\"\"\"\n",
    "    assert (df['CoC Number'].duplicated() == True).any() == False\n",
    "    pit_df = df.loc[df['CoC Number'].isin(\n",
    "        uniquecoc)].reset_index(drop='index').rename(columns={'CoC': 'CoC Number'})\n",
    "    pit_df[\"year\"] = year\n",
    "    return pit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_2022 = process_pit(pit_2022, 2022)\n",
    "pit_2021 = process_pit(pit_2021, 2021)\n",
    "pit_2020 = process_pit(pit_2020, 2020)\n",
    "pit_2019 = process_pit(pit_2019, 2019)\n",
    "pit_2018 = process_pit(pit_2018, 2018)\n",
    "pit_2017 = process_pit(pit_2017, 2017)\n",
    "pit_2016 = process_pit(pit_2016, 2016)\n",
    "pit_2015 = process_pit(pit_2015, 2015)\n",
    "pit_2014 = process_pit(pit_2014, 2014)\n",
    "pit_2013 = process_pit(pit_2013, 2013)\n",
    "pit_2012 = process_pit(pit_2012, 2012)\n",
    "pit_2011 = process_pit(pit_2011, 2011)\n",
    "pit_2010 = process_pit(pit_2010, 2010)\n",
    "pit_2009 = process_pit(pit_2009, 2009)\n",
    "pit_2008 = process_pit(pit_2008, 2008)\n",
    "pit_2007 = process_pit(pit_2007, 2007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of DataFrames\n",
    "pit = [\n",
    "    pit_2022,\n",
    "    pit_2021,\n",
    "    pit_2020,\n",
    "    pit_2019,\n",
    "    pit_2018,\n",
    "    pit_2017,\n",
    "    pit_2016,\n",
    "    pit_2015,\n",
    "    pit_2014,\n",
    "    pit_2013,\n",
    "    pit_2012,\n",
    "    pit_2011,\n",
    "    pit_2010,\n",
    "    pit_2009,\n",
    "    pit_2008,\n",
    "    pit_2007,]\n",
    "\n",
    "# Concatenate the dataframes\n",
    "pit_combined = pd.concat(pit, ignore_index=True)\n",
    "\n",
    "# # Confirm the resulting dataframe has no duplicates and null values\n",
    "check_dup_null(pit_combined)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5 Merge HIC and PIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hic_pit = pd.merge(hic_combined, pit_combined,\n",
    "                      on=['CoC Number', 'year'], how='outer', validate='1:1')\n",
    "\n",
    "check_dup_null(df_hic_pit)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Poverty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty = pd.read_excel(\n",
    "    'https://www2.census.gov/programs-surveys/saipe/datasets/time-series/model-tables/allpovu.xls',\n",
    "    header=[3],\n",
    "    usecols='A:D, E, I, M, Q, U, Y, AC, AG, AK, AO, AS, AW, BA, BE, BI',\n",
    ")\n",
    "poverty = poverty.rename(columns={'Poverty Universe, All Ages': '2021',\n",
    "                                  'Poverty Universe, All Ages.1': 2020,\n",
    "                                  'Poverty Universe, All Ages.2': 2019,\n",
    "                                  'Poverty Universe, All Ages.3': 2018,\n",
    "                                  'Poverty Universe, All Ages.4': 2017,\n",
    "                                  'Poverty Universe, All Ages.5': 2016,\n",
    "                                  'Poverty Universe, All Ages.6': 2015,\n",
    "                                  'Poverty Universe, All Ages.7': 2014,\n",
    "                                  'Poverty Universe, All Ages.8': 2013,\n",
    "                                  'Poverty Universe, All Ages.9': 2012,\n",
    "                                  'Poverty Universe, All Ages.10': 2011,\n",
    "                                  'Poverty Universe, All Ages.11': 2010,\n",
    "                                  'Poverty Universe, All Ages.12': 2009,\n",
    "                                  'Poverty Universe, All Ages.13': 2008,\n",
    "                                  'Poverty Universe, All Ages.14': 2007, })\n",
    "\n",
    "\n",
    "for i in range(len(poverty)):\n",
    "    if len(str(poverty.loc[i, \"State FIPS code\"])) == 1:\n",
    "        poverty.loc[i, \"State FIPS code\"] = '0' + \\\n",
    "            str(poverty.loc[i, \"State FIPS code\"])\n",
    "    else:\n",
    "        poverty.loc[i, \"State FIPS code\"] = str(\n",
    "            poverty.loc[i, \"State FIPS code\"])\n",
    "\n",
    "for i in range(len(poverty)):\n",
    "    if len(str(poverty.loc[i, \"County FIPS code\"])) == 1:\n",
    "        poverty.loc[i, \"County FIPS code\"] = '00' + \\\n",
    "            str(poverty.loc[i, \"County FIPS code\"])\n",
    "    elif len(str(poverty.loc[i, \"County FIPS code\"])) == 2:\n",
    "        poverty.loc[i, \"County FIPS code\"] = '0' + \\\n",
    "            str(poverty.loc[i, \"County FIPS code\"])\n",
    "    else:\n",
    "        poverty.loc[i, \"County FIPS code\"] = str(\n",
    "            poverty.loc[i, \"County FIPS code\"])\n",
    "\n",
    "poverty['fips'] = poverty['State FIPS code'] + \\\n",
    "    poverty['County FIPS code']\n",
    "\n",
    "poverty_filtered = poverty[poverty.fips.isin(fips_coc.keys())].drop(\n",
    "    ['State FIPS code', 'County FIPS code'], axis=1).reset_index(drop='index')\n",
    "poverty_filtered[\"CoC Number\"] = poverty_filtered[\"fips\"].map(fips_coc)\n",
    "\n",
    "poverty_grouped = poverty_filtered.groupby(\n",
    "    'CoC Number').apply('sum', numeric_only=True)\n",
    "poverty_grouped.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df(dataframe: object, col_name: str):\n",
    "    # Transform the dataframe to matching format as the previous dataframes\n",
    "    df_melted = pd.melt(dataframe, id_vars=[\n",
    "                            'CoC Number'], var_name='year', value_name=col_name)\n",
    "    # Convert the 'Year' column to integer type\n",
    "    df_melted['year'] = df_melted['year'].astype(int)\n",
    "    return df_melted\n",
    "\n",
    "poverty_melted = melt_df(poverty_grouped, 'poverty')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge poverty data with HIC & PIT\n",
    "df_hic_pit_pov = pd.merge(df_hic_pit, poverty_melted, on=[\n",
    "                          'CoC Number', 'year'], how='outer', validate='1:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the resulting dataframe has no duplicates and null values\n",
    "def check_dup_null_to2021(df: object, col: object, num_null_allowed: int) -> None:\n",
    "    assert df.duplicated().sum() == 0, \"Found duplicates in the dataframe.\"\n",
    "    assert df.isnull().sum().sum() == num_null_allowed, \"Found null values in the dataframe.\"\n",
    "    assert (df.groupby(\"CoC Number\")[[\"permanent_housing\", \"homeless\"]].count() == 16).all().all(\n",
    "    ) == True, \"Missing certain years of data for permanent housing and homeless count. Review yearly data for each CoC.\"\n",
    "    # Because certain dataset only contains data from 2007-2021,\n",
    "    # null values are expected from these data sources for 2022\n",
    "    assert (df.groupby(\"CoC Number\")[col].count() == 15).all().all(\n",
    "    ) == True, \"Missing certain years of data from 2007 to 2021. Review yearly data for each CoC.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dup_null_to2021(df_hic_pit_pov, \"poverty\", 19)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Unemployment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = pd.read_excel(\n",
    "    'https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xlsx?v=2298.6',\n",
    "    header=[4],\n",
    "    usecols='A, AL, AP, AT, AX, BB, BF, BJ, BN, BR, BV, BZ, CD, CH, CL, CP',\n",
    "    converters={'FIPS_code': str}\n",
    ")\n",
    "unemployment = unemployment.rename(columns={'FIPS_code': 'fips',\n",
    "                                            'Unemployment_rate_2007': 2007,\n",
    "                                            'Unemployment_rate_2008': 2008,\n",
    "                                            'Unemployment_rate_2009': 2009,\n",
    "                                            'Unemployment_rate_2010': 2010,\n",
    "                                            'Unemployment_rate_2011': 2011,\n",
    "                                            'Unemployment_rate_2012': 2012,\n",
    "                                            'Unemployment_rate_2013': 2013,\n",
    "                                            'Unemployment_rate_2014': 2014,\n",
    "                                            'Unemployment_rate_2015': 2015,\n",
    "                                            'Unemployment_rate_2016': 2016,\n",
    "                                            'Unemployment_rate_2017': 2017,\n",
    "                                            'Unemployment_rate_2018': 2018,\n",
    "                                            'Unemployment_rate_2019': 2019,\n",
    "                                            'Unemployment_rate_2020': 2020,\n",
    "                                            'Unemployment_rate_2021': 2021,\n",
    "                                            })\n",
    "unemployment = unemployment[unemployment.fips.isin(\n",
    "    fips_coc.keys())].reset_index(drop='index')\n",
    "unemployment[\"CoC Number\"] = unemployment[\"fips\"].map(fips_coc)\n",
    "\n",
    "unemployment_grouped = unemployment.groupby(\n",
    "    'CoC Number').apply('sum', numeric_only=True)\n",
    "unemployment_grouped.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_melted = melt_df(unemployment_grouped, \"unemployment_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge poverty data with HIC & PIT\n",
    "df_hic_pit_pov_unemp = pd.merge(df_hic_pit_pov, unemployment_melted, on=[\n",
    "    'CoC Number', 'year'], how='outer', validate=\"1:1\")\n",
    "\n",
    "check_dup_null_to2021(df_hic_pit_pov_unemp, [\"poverty\", \"unemployment_rate\"], 19*2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Mortality Data (Suicide & Drug Related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "mort2021us.txt                                 2022-12-12 10:12:52   2843666280\n",
      "Extracting all the files now...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "That compression method is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# extracting all the files\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mExtracting all the files now...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[39mzip\u001b[39;49m\u001b[39m.\u001b[39;49mextractall()\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:1643\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[1;32m   1642\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:1696\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         os\u001b[39m.\u001b[39mmkdir(targetpath)\n\u001b[1;32m   1694\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m-> 1696\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(member, pwd\u001b[39m=\u001b[39;49mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[1;32m   1697\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m   1698\u001b[0m     shutil\u001b[39m.\u001b[39mcopyfileobj(source, target)\n\u001b[1;32m   1700\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:1569\u001b[0m, in \u001b[0;36mZipFile.open\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1567\u001b[0m         pwd \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1569\u001b[0m     \u001b[39mreturn\u001b[39;00m ZipExtFile(zef_file, mode, zinfo, pwd, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1570\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1571\u001b[0m     zef_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:800\u001b[0m, in \u001b[0;36mZipExtFile.__init__\u001b[0;34m(self, fileobj, mode, zipinfo, pwd, close_fileobj)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m=\u001b[39m zipinfo\u001b[39m.\u001b[39mcompress_size\n\u001b[1;32m    798\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m=\u001b[39m zipinfo\u001b[39m.\u001b[39mfile_size\n\u001b[0;32m--> 800\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor \u001b[39m=\u001b[39m _get_decompressor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compress_type)\n\u001b[1;32m    802\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:699\u001b[0m, in \u001b[0;36m_get_decompressor\u001b[0;34m(compress_type)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_decompressor\u001b[39m(compress_type):\n\u001b[0;32m--> 699\u001b[0m     _check_compression(compress_type)\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m compress_type \u001b[39m==\u001b[39m ZIP_STORED:\n\u001b[1;32m    701\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/zipfile.py:679\u001b[0m, in \u001b[0;36m_check_compression\u001b[0;34m(compression)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    677\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCompression requires the (missing) lzma module\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    678\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThat compression method is not supported\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: That compression method is not supported"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "from zipfile import ZipFile\n",
    "  \n",
    "# specifying the zip file name\n",
    "file_name = \"mort2021us.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11GU010</td>\n",
       "      <td>3GU</td>\n",
       "      <td>GU010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GU</td>\n",
       "      <td>3101</td>\n",
       "      <td>M1066</td>\n",
       "      <td>391909  3S1</td>\n",
       "      <td>2021N2CN</td>\n",
       "      <td>X74 429</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>X74</td>\n",
       "      <td>S019 T141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11GU010</td>\n",
       "      <td>3GU</td>\n",
       "      <td>GU010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>YY</td>\n",
       "      <td>1101</td>\n",
       "      <td>M1082</td>\n",
       "      <td>422210  1M7</td>\n",
       "      <td>2021U7CN</td>\n",
       "      <td>E149159</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>E149</td>\n",
       "      <td>F179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11GU010</td>\n",
       "      <td>3GU</td>\n",
       "      <td>GU010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GU</td>\n",
       "      <td>2101</td>\n",
       "      <td>M1076</td>\n",
       "      <td>412110  4M1</td>\n",
       "      <td>2021U7BU</td>\n",
       "      <td>C259088</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>C259</td>\n",
       "      <td>C780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11GU010</td>\n",
       "      <td>3GU</td>\n",
       "      <td>GU010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GU</td>\n",
       "      <td>9101</td>\n",
       "      <td>M1054</td>\n",
       "      <td>361607  2M2</td>\n",
       "      <td>2021U7BN</td>\n",
       "      <td>C169078</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>C169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11GU010</td>\n",
       "      <td>3GU</td>\n",
       "      <td>GU010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>YY</td>\n",
       "      <td>4101</td>\n",
       "      <td>M1068</td>\n",
       "      <td>391909  1M7</td>\n",
       "      <td>2021U7BN</td>\n",
       "      <td>U071054</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>U071</td>\n",
       "      <td>J189 J960</td>\n",
       "      <td>J961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35566</th>\n",
       "      <td>11VI999</td>\n",
       "      <td>9VI</td>\n",
       "      <td>VI030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YY</td>\n",
       "      <td>3112</td>\n",
       "      <td>M1085</td>\n",
       "      <td>432311  1D3</td>\n",
       "      <td>2021U7BN</td>\n",
       "      <td>C80 125</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>C80</td>\n",
       "      <td>I469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35567</th>\n",
       "      <td>11VI999</td>\n",
       "      <td>9VI</td>\n",
       "      <td>VI030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YY</td>\n",
       "      <td>2112</td>\n",
       "      <td>F1092</td>\n",
       "      <td>442411  4S5</td>\n",
       "      <td>2021U7BN</td>\n",
       "      <td>I251215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35568</th>\n",
       "      <td>11VI999</td>\n",
       "      <td>9VI</td>\n",
       "      <td>VI030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YY</td>\n",
       "      <td>5112</td>\n",
       "      <td>M1084</td>\n",
       "      <td>422210  4W6</td>\n",
       "      <td>2021U7BN</td>\n",
       "      <td>I251215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35569</th>\n",
       "      <td>11VI999</td>\n",
       "      <td>9VI</td>\n",
       "      <td>VI030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YY</td>\n",
       "      <td>3101</td>\n",
       "      <td>M1069</td>\n",
       "      <td>391909  4M3</td>\n",
       "      <td>2021U7BY</td>\n",
       "      <td>I251215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35570</th>\n",
       "      <td>11VI999</td>\n",
       "      <td>9VI</td>\n",
       "      <td>VI030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>2103</td>\n",
       "      <td>M1041</td>\n",
       "      <td>341406  2S5</td>\n",
       "      <td>2021N1UY</td>\n",
       "      <td>V870397</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>V870</td>\n",
       "      <td>I469 S199</td>\n",
       "      <td>S270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35571 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1      2    3   4     5      6            7         8   \\\n",
       "0      11GU010  3GU  GU010  3.0  GU  3101  M1066  391909  3S1  2021N2CN   \n",
       "1      11GU010  3GU  GU010  3.0  YY  1101  M1082  422210  1M7  2021U7CN   \n",
       "2      11GU010  3GU  GU010  3.0  GU  2101  M1076  412110  4M1  2021U7BU   \n",
       "3      11GU010  3GU  GU010  3.0  GU  9101  M1054  361607  2M2  2021U7BN   \n",
       "4      11GU010  3GU  GU010  3.0  YY  4101  M1068  391909  1M7  2021U7BN   \n",
       "...        ...  ...    ...  ...  ..   ...    ...          ...       ...   \n",
       "35566  11VI999  9VI  VI030  NaN  YY  3112  M1085  432311  1D3  2021U7BN   \n",
       "35567  11VI999  9VI  VI030  NaN  YY  2112  F1092  442411  4S5  2021U7BN   \n",
       "35568  11VI999  9VI  VI030  NaN  YY  5112  M1084  422210  4W6  2021U7BN   \n",
       "35569  11VI999  9VI  VI030  NaN  YY  3101  M1069  391909  4M3  2021U7BY   \n",
       "35570  11VI999  9VI  VI030  NaN  ZZ  2103  M1041  341406  2S5  2021N1UY   \n",
       "\n",
       "            9   ...   15   16 17    18         19    20   21   22   23  24  \n",
       "0      X74 429  ...  NaN  NaN  3   X74  S019 T141   NaN  NaN  NaN  100  12  \n",
       "1      E149159  ...  NaN  NaN  2  E149       F179   NaN  NaN  NaN  100   6  \n",
       "2      C259088  ...  NaN  NaN  2  C259       C780   NaN  NaN  NaN  100  12  \n",
       "3      C169078  ...  NaN  NaN  1  C169        NaN   NaN  NaN  NaN  100  12  \n",
       "4      U071054  ...  NaN  NaN  4  U071  J189 J960  J961  NaN  NaN  100  14  \n",
       "...        ...  ...  ...  ... ..   ...        ...   ...  ...  ...  ...  ..  \n",
       "35566  C80 125  ...  NaN  NaN  2   C80       I469   NaN  NaN  NaN  100   2  \n",
       "35567  I251215  ...  NaN  NaN  1  I251        NaN   NaN  NaN  NaN  100   2  \n",
       "35568  I251215  ...  NaN  NaN  1  I251        NaN   NaN  NaN  NaN  100   3  \n",
       "35569  I251215  ...  NaN  NaN  1  I251        NaN   NaN  NaN  NaN  100   2  \n",
       "35570  V870397  ...  NaN  NaN  4  V870  I469 S199  S270  NaN  NaN  210   1  \n",
       "\n",
       "[35571 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mortality = pd.read_fwf('mort2021ps.txt', \n",
    "            header=None, \n",
    "            ).dropna(axis=\"columns\", how='all')\n",
    "mortality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUMLEV', 'STATE', 'COUNTY', 'STNAME', 'CTYNAME', 'YEAR', 'AGEGRP',\n",
       "       'TOT_POP', 'TOT_MALE', 'TOT_FEMALE', 'WA_MALE', 'WA_FEMALE', 'BA_MALE',\n",
       "       'BA_FEMALE', 'IA_MALE', 'IA_FEMALE', 'AA_MALE', 'AA_FEMALE', 'NA_MALE',\n",
       "       'NA_FEMALE', 'TOM_MALE', 'TOM_FEMALE', 'WAC_MALE', 'WAC_FEMALE',\n",
       "       'BAC_MALE', 'BAC_FEMALE', 'IAC_MALE', 'IAC_FEMALE', 'AAC_MALE',\n",
       "       'AAC_FEMALE', 'NAC_MALE', 'NAC_FEMALE', 'NH_MALE', 'NH_FEMALE',\n",
       "       'NHWA_MALE', 'NHWA_FEMALE', 'NHBA_MALE', 'NHBA_FEMALE', 'NHIA_MALE',\n",
       "       'NHIA_FEMALE', 'NHAA_MALE', 'NHAA_FEMALE', 'NHNA_MALE', 'NHNA_FEMALE',\n",
       "       'NHTOM_MALE', 'NHTOM_FEMALE', 'NHWAC_MALE', 'NHWAC_FEMALE',\n",
       "       'NHBAC_MALE', 'NHBAC_FEMALE', 'NHIAC_MALE', 'NHIAC_FEMALE',\n",
       "       'NHAAC_MALE', 'NHAAC_FEMALE', 'NHNAC_MALE', 'NHNAC_FEMALE', 'H_MALE',\n",
       "       'H_FEMALE', 'HWA_MALE', 'HWA_FEMALE', 'HBA_MALE', 'HBA_FEMALE',\n",
       "       'HIA_MALE', 'HIA_FEMALE', 'HAA_MALE', 'HAA_FEMALE', 'HNA_MALE',\n",
       "       'HNA_FEMALE', 'HTOM_MALE', 'HTOM_FEMALE', 'HWAC_MALE', 'HWAC_FEMALE',\n",
       "       'HBAC_MALE', 'HBAC_FEMALE', 'HIAC_MALE', 'HIAC_FEMALE', 'HAAC_MALE',\n",
       "       'HAAC_FEMALE', 'HNAC_MALE', 'HNAC_FEMALE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_2021 = pd.read_csv(\n",
    "    'https://www2.census.gov/programs-surveys/popest/datasets/2020-2021/counties/asrh/cc-est2021-all.csv', \n",
    "    encoding=\"ISO-8859-1\",\n",
    "    # usecols=['COUNTY', 'STNAME','YEAR']\n",
    "    )\n",
    "pop_2021.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
